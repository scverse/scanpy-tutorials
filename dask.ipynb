{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{canonical-tutorial} tutorials/experimental/dask\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `dask` with Scanpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{warning} ðŸ”ª **Beware sharp edges!** ðŸ”ª\n",
    "\n",
    "`dask` support in `scanpy` is new and highly experimental!\n",
    "\n",
    "Many functions in `scanpy` **do not** support `dask` and may exhibit unexpected behaviour if dask arrays are passed to them. Stick to what's outlined in this tutorial and you should be fine!\n",
    "\n",
    "Please report any issues you run into over on the issue tracker.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`dask`](https://www.dask.org) is a popular out-of-core, distributed array processing library that scanpy is beginning to support. Here we walk through a quick tutorial of using `dask` in a simple analysis task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook relies on optional dependencies in dask, `sklearn-ann` and `annoy`. Install them with:\n",
    "\n",
    "```sh\n",
    "pip install -U \"scanpy[dask]\" \"dask[distributed,diagnostics]\" sklearn-ann annoy\n",
    "```\n",
    "\n",
    "(`scanpy[dask]` means to install scanpy together with `dask[array]`, but will also always make sure that a compatible dask version is selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import dask.distributed as dd\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import h5py\n",
    "\n",
    "sc.logging.print_header()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll be working with a moderately large dataset of 1.4 million cells taken from: [COVID-19 immune features revealed by a large-scale single-cell transcriptome atlas](https://cellxgene.cziscience.com/collections/0a839c4b-10d0-4d64-9272-684c49a2c8ba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, path: Path) -> None:\n",
    "    \"\"\"Download a file from `url` and save it to `path`, showing a progress bar.\"\"\"\n",
    "    from tqdm.autonotebook import tqdm\n",
    "    from urllib.request import urlretrieve\n",
    "\n",
    "    pb = tqdm(unit=\"B\", unit_scale=True, unit_divisor=1024)\n",
    "\n",
    "    def update(b: int = 1, bsize: int = 1, tsize: int | None = None):\n",
    "        if tsize is not None:\n",
    "            pb.total = tsize\n",
    "        return pb.update(b * bsize - pb.n)\n",
    "\n",
    "    try:\n",
    "        with pb:\n",
    "            urlretrieve(url, path, reporthook=update)\n",
    "    except BaseException:\n",
    "        path.unlink(missing_ok=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_atlas_path = Path(\"data/cell_atlas.h5ad\")\n",
    "cell_atlas_path.parent.mkdir(exist_ok=True)\n",
    "if not cell_atlas_path.exists():\n",
    "    download(\n",
    "        \"https://datasets.cellxgene.cziscience.com/82eac9c1-485f-4e21-ab21-8510823d4f6e.h5ad\",\n",
    "        cell_atlas_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on using distributed computing via `dask`, please see their [documentation](https://docs.dask.org/en/stable/deploying-python.html). In short, one needs to define both a cluster and a client to have some degree of control over the compute resources dask will use. It's very likely you will have to tune the number of workers and amount of memory per worker along with your chunk sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dd.LocalCluster(n_workers=3)\n",
    "client = dd.Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "In this notebook we will be demonstrating some computations in scanpy that use `scipy.sparse` classes within each dask chunk. Be aware that this is currently poorly supported by `dask`, and that if you want to interact with the `dask` arrays in any way other than though the `anndata` and `scanpy` libraries you will likely need to densify each chunk.\n",
    "\n",
    "All operations in `scanpy` and `anndata` that work with sparse chunks also work with dense chunks.\n",
    "\n",
    "The advantage of using sparse chunks are:\n",
    "\n",
    "- The ability to work with fewer, larger chunks\n",
    "- Accelerated computations per chunk (e.g. don't need to `sum` all those extra zeros)\n",
    "\n",
    "You can convert from `sparse` to `dense` chunks via:\n",
    "\n",
    "```python\n",
    "X = X.map_blocks(lambda x: x.toarray(), dtype=X.dtype, meta=np.array([]))\n",
    "```\n",
    "\n",
    "And in reverse:\n",
    "\n",
    "```python\n",
    "X = X.map_blocks(sparse.csr_matrix)\n",
    "```\n",
    "\n",
    "Note that you will likely have to work with smaller chunks when doing this, via a rechunking operation. We suggest using a factor of the larger chunk size to achieve the most efficient rechunking.\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARSE_CHUNK_SIZE = 100_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask provides extensive tooling for monitoring your computation. You can access that via the dashboard started when using any of their distributed clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll convert the `X` representation to `dask` using `anndata.experimental.read_elem_as_dask`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file we've retrieved from cellxgene has already been processed. Since this tutorial is demonstrating processing from counts, we're just going to access the counts matrix and annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with h5py.File(cell_atlas_path, \"r\") as f:\n",
    "    adata = ad.AnnData(\n",
    "        obs=ad.io.read_elem(f[\"obs\"]),\n",
    "        var=ad.io.read_elem(f[\"var\"]),\n",
    "    )\n",
    "    adata.X = ad.experimental.read_elem_as_dask(\n",
    "        f[\"raw/X\"], chunks=(SPARSE_CHUNK_SIZE, adata.shape[1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've optimized a number of scanpy functions to be completely lazy. That means it will look like nothing is computed when you call an operation on a dask array, but only later when you hit compute.\n",
    "\n",
    "In some cases it's currently unavoidable to skip all computation, and these cases will kick off compute for all the delayed operations immediately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "adata.layers[\"counts\"] = adata.X.copy()  # Making sure we keep access to the raw counts\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.pp.log1p(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly variable genes needs to add entries into `obs`, which currently does not support lazy column. So computation will occur immediately on call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.pp.highly_variable_genes(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scanpy 1.11+ supports computing PCA on `dask` arrays with sparse chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.pp.pca(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While most of the PCA computation runs immediately, the last step (computing the observation loadings) is lazy, so must be triggered manually to avoid recomputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "adata.obsm[\"X_pca\"] = adata.obsm[\"X_pca\"].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've computed our PCA let's take a look at it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.pca(adata, color=\"majorType\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further support for `dask` is a work in progress. However, many operations past this point can work with the dimensionality reduction directly in memory. With scanpy `1.10` many of these operations can be accelerated to make working with large datasets significantly easier. For example:\n",
    "\n",
    "- Using alternative KNN backends for faster neighbor calculation {doc}`/how-to/knn-transformers`\n",
    "- Using the `igraph` backend for clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn_ann.kneighbors.annoy import AnnoyTransformer  # noqa: E402\n",
    "\n",
    "transformer = AnnoyTransformer(n_neighbors=15)\n",
    "sc.pp.neighbors(adata, transformer=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.tl.leiden(adata, flavor=\"igraph\", n_iterations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP computation can still be rather slow, taking longer than the rest of this notebook combined:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sc.tl.umap(adata)\n",
    "sc.pl.umap(adata, color=[\"leiden\", \"majorType\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
